{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PJM Hourly Energy Consumption Case\n",
    "\n",
    "PJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.\n",
    "\n",
    "The hourly power consumption data comes from PJM's website and are in megawatts (MW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Training Step - By Sabrina Otoni da Silva - 2024/04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, classification_report, ConfusionMatrixDisplay, accuracy_score, confusion_matrix\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras.layers as L\n",
    "# from tensorflow.keras import optimizers, Sequential, Model\n",
    "# from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('../data/d02_intermediate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{datapath}/pjme_train.csv')\n",
    "df.set_index('datetime', inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.sort_index(inplace=True)\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporalize(X, y, lookback):\n",
    "    '''\n",
    "    To convert input data into 3-D\n",
    "    array as required for LSTM network.\n",
    "    '''\n",
    "    output_X = []\n",
    "    output_y = []\n",
    "    for i in range(len(X)-lookback-1):\n",
    "        t = []\n",
    "        for j in range(1,lookback+1):\n",
    "            t.append(X[[(i+j+1)], :])\n",
    "        output_X.append(t)\n",
    "        output_y.append(y[i+lookback+1])\n",
    "        \n",
    "    return output_X, output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = df.shape[1] - 1\n",
    "timesteps = 24\n",
    "\n",
    "X_train, y_train = temporalize(X = np.array(df[['hour', 'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'day', 'weekofyear', 'lag1', 'lag2', 'lag3']]), \n",
    "                   y = np.array(df[['pjme_mw']]), \n",
    "                   lookback = timesteps)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape(X_train.shape[0], timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX = MinMaxScaler(feature_range=(0, 1))\n",
    "scalerX.fit(X_train)\n",
    "X_train= scalerX.transform(X_train)\n",
    "X_train.shape\n",
    "# scalerY = MinMaxScaler(feature_range=(0, 1))\n",
    "# scalerY.fit(y_train)\n",
    "# y_train = scalerY.transform(y_train).reshape(-1, y_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_model = Sequential()\n",
    "# lstm_model.add(L.LSTM(100, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "# lstm_model.add(L.RepeatVector(X_train.shape[1]))\n",
    "# lstm_model.add(L.LSTM(100, activation='relu', return_sequences=True))\n",
    "# lstm_model.add(L.TimeDistributed(L.Dense(1)))\n",
    "# lstm_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serie_size =  X_train.shape[1]\n",
    "# n_features =  X_train.shape[2]\n",
    "\n",
    "# encoder_decoder = Sequential()\n",
    "# encoder_decoder.add(L.LSTM(serie_size, activation='relu', input_shape=(serie_size, n_features), return_sequences=True))\n",
    "# encoder_decoder.add(L.LSTM(6, activation='relu', return_sequences=True))\n",
    "# encoder_decoder.add(L.LSTM(1, activation='relu'))\n",
    "# encoder_decoder.add(L.RepeatVector(serie_size))\n",
    "# encoder_decoder.add(L.LSTM(serie_size, activation='relu', return_sequences=True))\n",
    "# encoder_decoder.add(L.LSTM(6, activation='relu', return_sequences=True))\n",
    "# encoder_decoder.add(L.TimeDistributed(L.Dense(1)))\n",
    "# encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_decoder.compile(loss='mse', optimizer=optimizers.Adam(0.0001))\n",
    "\n",
    "# encoder_decoder_history = encoder_decoder.fit(X_train, X_train, \n",
    "#                                               batch_size=128, \n",
    "#                                               epochs=20, \n",
    "#                                               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpt_vector_layer = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[3].output)\n",
    "# time_dist_layer = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[5].output)\n",
    "# encoder_decoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded = encoder.predict(X_train)\n",
    "# print('Encoded time-series shape', train_encoded.shape)\n",
    "# print('Encoded time-series sample', train_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['encoded'] = train_encoded\n",
    "# df['label'] = y_train\n",
    "\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = EarlyStopping(patience=10, verbose=0, min_delta=0.001, monitor='loss', mode='auto', restore_best_weights=True)\n",
    "# red_lr = LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "\n",
    "# hist = lstm_model.fit(\n",
    "#     x=X_train,\n",
    "#     y=y_train,\n",
    "#     batch_size=100,\n",
    "#     shuffle=False,\n",
    "#     epochs=300,\n",
    "#     verbose=1,\n",
    "#     callbacks=[es, red_lr]\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
