{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PJM Hourly Energy Consumption Case\n",
    "\n",
    "PJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.\n",
    "\n",
    "The hourly power consumption data comes from PJM's website and are in megawatts (MW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Training Step - By Sabrina Otoni da Silva - 2024/04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import tensorflow.keras.layers as L\n",
    "from keras_tuner import Hyperband\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('../data/d02_intermediate')\n",
    "modelpath = Path('../model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{datapath}/pjme_train.csv')\n",
    "df.set_index('datetime', inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.sort_index(inplace=True)\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporalize(X, y, lookback):\n",
    "    '''\n",
    "    To convert input data into 3-D\n",
    "    array as required for LSTM network.\n",
    "    '''\n",
    "    output_X = []\n",
    "    output_y = []\n",
    "    for i in range(len(X)-lookback-1):\n",
    "        t = []\n",
    "        for j in range(1,lookback+1):\n",
    "            t.append(X[[(i+j+1)], :])\n",
    "        output_X.append(t)\n",
    "        output_y.append(y[i+lookback+1])\n",
    "        \n",
    "    return output_X, output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    '''\n",
    "    Flatten a 3D array.\n",
    "    Input\n",
    "    X - A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "\n",
    "    Output\n",
    "    flattened_X - A 2D array, sample x features.\n",
    "    '''\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # Sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "\n",
    "def scale(X, scaler):\n",
    "    '''\n",
    "    Scale 3D array.\n",
    "    Inputs\n",
    "    X - A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    scaler - A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize\n",
    "    \n",
    "    Output\n",
    "    X - Scaled 3D array.\n",
    "    '''\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = df.shape[1] - 1\n",
    "timesteps = 24\n",
    "\n",
    "X_train, y_train = temporalize(X = np.array(df[['hour', 'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'day', 'weekofyear', 'lag1', 'lag2', 'lag3']]), \n",
    "                   y = np.array(df[['pjme_mw']]), \n",
    "                   lookback = timesteps)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape(X_train.shape[0], timesteps, n_features)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = StandardScaler().fit(flatten(X_train))\n",
    "X_train = scale(X_train, scaler_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5, test_size=24*365*1, gap=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     # Encoder\n",
    "#     model.add(L.LSTM(hp.Int('encoder_lstm_1_units', min_value=64, max_value=256, step=32),\n",
    "#                      activation='relu',\n",
    "#                      input_shape=(timesteps, n_features),\n",
    "#                      return_sequences=True))\n",
    "#     model.add(L.LSTM(hp.Int('encoder_lstm_2_units', min_value=32, max_value=128, step=32),\n",
    "#                      activation='relu',\n",
    "#                      return_sequences=False))\n",
    "#     model.add(L.RepeatVector(timesteps))\n",
    "#     # Decoder\n",
    "#     model.add(L.LSTM(hp.Int('decoder_lstm_1_units', min_value=32, max_value=128, step=32),\n",
    "#                      activation='relu',\n",
    "#                      return_sequences=True))\n",
    "#     model.add(L.LSTM(hp.Int('decoder_lstm_2_units', min_value=64, max_value=256, step=32),\n",
    "#                      activation='relu',\n",
    "#                      return_sequences=True))\n",
    "#     model.add(L.TimeDistributed(L.Dense(1, activation='linear')))\n",
    "    \n",
    "#     # Compiling the model\n",
    "#     model.compile(optimizer=optimizers.Adam(\n",
    "#                       hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "#                   loss='mse')\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score = float('inf')\n",
    "# best_model = None\n",
    "\n",
    "# for fold, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "#     print(f\"Training on fold {fold+1}...\")\n",
    "#     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "#     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "#     tuner = Hyperband(\n",
    "#         build_model,\n",
    "#         objective='val_loss',\n",
    "#         max_epochs=40,\n",
    "#         hyperband_iterations=2,\n",
    "#         factor=3,\n",
    "#         directory=f'{modelpath}/keras_tuner',\n",
    "#         project_name='lstm_autoencoder'\n",
    "#     )\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "#     tuner.search(\n",
    "#         x=X_train_fold,\n",
    "#         y=y_train_fold,\n",
    "#         epochs=50,\n",
    "#         validation_data=(X_val_fold, y_val_fold),\n",
    "#         callbacks=[early_stopping],\n",
    "#         verbose=5\n",
    "#     )\n",
    "\n",
    "#     best_model_fold = tuner.get_best_models(num_models=1)[0]\n",
    "#     best_hyperparameters_fold = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "#     val_score_fold = best_model_fold.evaluate(X_val_fold, y_val_fold)\n",
    "\n",
    "#     if val_score_fold < best_score:\n",
    "#         best_score = val_score_fold\n",
    "#         best_model = best_model_fold\n",
    "\n",
    "#     best_model.save(f'{modelpath}/best_model.h5')\n",
    "    \n",
    "#     with open(f'{modelpath}/best_hyperparameters.pkl', 'wb') as f:\n",
    "#         pickle.dump(best_hyperparameters_fold, f)\n",
    "    \n",
    "#     print(f\"Validation score for fold {fold+1}: {val_score_fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_autoencoder():\n",
    "    model = Sequential()\n",
    "    # Encoder\n",
    "    model.add(L.LSTM(128, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
    "    model.add(L.LSTM(64, activation='relu', return_sequences=False))\n",
    "    model.add(L.RepeatVector(timesteps))\n",
    "    # Decoder\n",
    "    model.add(L.LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(L.LSTM(128, activation='relu', return_sequences=True))\n",
    "    model.add(L.TimeDistributed(L.Dense(1, activation='linear')))\n",
    "    \n",
    "    model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           71680     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 24, 64)            0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 24, 64)            33024     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 24, 128)           98816     \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 24, 1)             129       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 253057 (988.50 KB)\n",
      "Trainable params: 253057 (988.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_autoencoder = lstm_autoencoder()\n",
    "lstm_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# for fold, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "#     print(f\"Training on fold {fold+1}...\")\n",
    "#     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "#     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "history = lstm_autoencoder.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=113919,\n",
    "        #validation_data=(X_val_fold, y_val_fold),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # val_loss = lstm_autoencoder.evaluate(X_val_fold, y_val_fold)\n",
    "    # print(\"Validation Loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
